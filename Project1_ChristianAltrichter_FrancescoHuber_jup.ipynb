{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u014401kRTA9"
      },
      "outputs": [],
      "source": [
        "# Data Analytics Project 1 - Christian Altrichter, Francesco Huber \n",
        "# Project Nr. 15 Suspicious Passages\n",
        "\n",
        "# ALL CODE WRITTEN AND PRODUCED BY THE TEAM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import csv\n",
        "import kshingle as ks\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import reduce_lines as rd\n",
        "import re\n",
        "import string\n",
        "\n",
        "from datasketch import MinHashLSH, MinHash, MinHashLSHForest\n",
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "id": "syGwyV3kSkno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CORE COMPUTATIONS"
      ],
      "metadata": {
        "id": "pzOyLkYSsryw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CORE COMPUTATIONS - Core statistical computation on the text files\n",
        "\n",
        "def get_paths_from_current_dir(input_path, ending = \"txt\"):\n",
        "    \"\"\"\n",
        "        Walks the current directory to extract all text files as paths for preprocessing\n",
        "\n",
        "    Args:\n",
        "        input_path (String): String of path, to be passed and processed then later by extract_data\n",
        "        ending (String): Defines which files should be extracted based on the ending of the files. \n",
        "\n",
        "    Returns:\n",
        "        list: returns a list of all paths to the individual files\n",
        "    \"\"\"\n",
        "    list_of_paths = []\n",
        "    for root, dirs, files in os.walk(input_path, topdown=False):\n",
        "        for name in files:\n",
        "            if name.endswith(ending):  \n",
        "                list_of_paths.append((os.path.join(root, name)))\n",
        "    list_of_paths.sort()\n",
        "    print(list_of_paths)\n",
        "    return list_of_paths\n",
        "\n",
        "\n",
        "def text_processor(data_file):\n",
        "    \"\"\"\n",
        "        extracts the following information from each text:\n",
        "            number of lines, unique vocabularies, the entire vocabulary list, \n",
        "            unique characters, all characters and the text as string\n",
        "    Args:\n",
        "        data_file (String): Text transformed as string \n",
        "\n",
        "    Returns:\n",
        "        int, set, list, set, list, string: returns a tuple of the relevant information of the \n",
        "        texts. \n",
        "    \"\"\"\n",
        "    line_counter = 0\n",
        "    unique_char = set()\n",
        "    characters = []\n",
        "    unique_vocab = set()\n",
        "    vocabulary = []\n",
        "    text = \"\"\n",
        "    word = \"\"\n",
        "    for line in data_file:\n",
        "        for char in line:\n",
        "            unique_char.update(char)\n",
        "            characters.append(char)\n",
        "        line_counter += 1\n",
        "        text += \" \" + line.strip()\n",
        "    \n",
        "    vocabulary = text.split(\" \")\n",
        "    unique_vocab.update(vocabulary)\n",
        "    return line_counter, unique_vocab, vocabulary,  unique_char, characters, text.lower()\n",
        "\n",
        "\n",
        "def process_all(root_dir):\n",
        "    \"\"\" \n",
        "        Processes the text files with the above methods and transforms the information into a usable\n",
        "        dataframe\n",
        "    \n",
        "    Args:\n",
        "        path_to_text_file (String): Takes as argument the root source directory to be processed.\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: returns a data frame summarizing the individual statistics\n",
        "    \"\"\"\n",
        "    list_of_paths = get_paths_from_current_dir(root_dir)\n",
        "    print(\"process all order\", list_of_paths)\n",
        "    doc_info = pd.DataFrame(columns=['File Name', \"Line Counter\", \"Length of unique vocabulary\", \"Length of all vocabulary\", \"Length of unique characters\", \"Length of all characters\"])\n",
        "    doc_raw = []\n",
        "\n",
        "    for i, elem in enumerate(list_of_paths):\n",
        "        file_name = elem.split(\"/\")[-1].split(\".\")[0]\n",
        "        line_counter, unique_vocab, vocabulary,  unique_char, characters, text = text_processor(open(elem))\n",
        "        doc_raw.append([file_name, line_counter, unique_vocab, vocabulary,  unique_char, characters, text])\n",
        "        doc_info.loc[i] = file_name, line_counter, len(unique_vocab), len(vocabulary), len(unique_char), len(characters)\n",
        "    \n",
        "    return doc_info, doc_raw\n",
        "\n",
        "def populate_CSV(dataFrame, fileName):\n",
        "    \"\"\"\n",
        "        Populates a XLS file from the data frame to be used then for visualization purposes in Tableau\n",
        "\n",
        "    Args:\n",
        "        dataFrame (pandas.DataFrame): A populated data frame with all texts information\n",
        "        fileName (String): Specifies how the files should be saved in the current directory \n",
        "    \"\"\"\n",
        "    dataFrame.to_csv(fileName + \".csv\")\n",
        "    \n",
        "\n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sampple core computation execution that will give you the basic statistical analysis of the text files\n",
        "# NOTE: THE INPUT PATH MUST BE UPDATED TO THE RESPECTIVE CORPUS OF SUSPICIOUS PASSAGES\n",
        "\n",
        "# input_path_original = \"./31.Suspicious_Passages/Corpus\"\n",
        "# data_frame_original_data, raw_orig = process_all(input_path_original)\n",
        "\n",
        "\n",
        "# dataframe_list = []\n",
        "# dataframe_list.append(data_frame_original_data)\n",
        "\n",
        "\n",
        "# rawdata_list = []\n",
        "# rawdata_list.append(raw_orig)\n",
        "\n",
        "\n",
        "# populate_CSV(data_frame_original_data, \"clean_files_0\")\n",
        "\n",
        "# print(\"Statistical analysis complete\")\n",
        "\n",
        "# df1 = pd.read_csv('clean_files_0.csv')\n",
        "# print(df1.describe())\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "W6sRvfCvRffk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REDUCE LINES"
      ],
      "metadata": {
        "id": "pbdcB8P0tXXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REDUCE LINES\n",
        "\n",
        "# Clean files of unncesessary passages (e.g. table of content, tables etc.) based on average word count per line.\n",
        "# Thus, as example - if we reduce the text by 0.5, then we remove all lines in which the word count is 50% less than \n",
        "# the mean count of word count per line\n",
        "\n",
        "\n",
        "def get_and_process_text(text_name):\n",
        "    \"\"\"\n",
        "    Opens Text, removes punctuation and capitalization then counts chars per line,\n",
        "    produces two lists, one for char per line and one with contents of the line\n",
        "    :param text_name: string corresponding to name of the source text\n",
        "    :return: lines: a list of lines appended as strings\n",
        "            line_count: a list of integers representing the amount of char per line\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    line_count = []\n",
        "    text = open(text_name)\n",
        "\n",
        "    for line in text:\n",
        "        line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "        line = line.replace('\\t', ' ')\n",
        "        line = re.sub(\"\\s\\s+\", \" \", line)\n",
        "        lines.append(line.lower())\n",
        "        line_count.append(len(line))\n",
        "\n",
        "    return lines, line_count\n",
        "\n",
        "\n",
        "def get_frame(line_count, mean_amount=1.0):\n",
        "    \"\"\"\n",
        "    Produces a DataFrame, removing lines that do not pass our threshold\n",
        "    :param line_count: a list of integers\n",
        "           mean_amount: float, mean multiplier\n",
        "\n",
        "    :return: edited_frame: a dataframe filtered by valid lines\n",
        "        \"\"\"\n",
        "    frame = pd.DataFrame(data=line_count, columns=[\"char_count\"])\n",
        "    mean_frame = frame[\"char_count\"].mean()\n",
        "    std_frame = frame[\"char_count\"].std()\n",
        "    edited_frame = frame[frame.char_count > (mean_frame * mean_amount)]\n",
        "    return edited_frame\n",
        "\n",
        "\n",
        "def plot_frame(frame_name, fig_size=None):\n",
        "    \"\"\"\n",
        "    produces a bar chart\n",
        "    :param frame_name: string name of our frame\n",
        "    :param fig_size: touple(int,int) size of our figure plotted\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=fig_size)\n",
        "    plt.bar(np.arange(len(frame_name['char_count'].values)),\n",
        "            frame_name['char_count'].values, align='center', alpha=1, width=1.0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_text(text_name, mean_amount):\n",
        "    \"\"\"\n",
        "    Compose Edited texts, iterate over all txt files and cut contents\n",
        "    Finally saving the new file\n",
        "    :param text_name: string, source file name\n",
        "    :param mean_amount: float, mean multiplier\n",
        "    :return: file, the created reference file\n",
        "    \"\"\"\n",
        "    line_content, char_amount = get_and_process_text(text_name)\n",
        "    frame = get_frame(char_amount, mean_amount)\n",
        "\n",
        "    # get valid line ids and compose actual lines\n",
        "    text = ''\n",
        "    for idx in frame.iterrows():\n",
        "        cleaned_line = line_content[idx[0]]\n",
        "        text += cleaned_line\n",
        "\n",
        "    # creates a new file called 'xxx'.txt and write some text into it\n",
        "    number_name = (text_name.split('.')[-2])[-5:] # maintain only number\n",
        "    new_file_name = number_name + '_clean_' + str(int(mean_amount * 100)) + '%_mean.txt'\n",
        "    with open(new_file_name, 'w') as file:\n",
        "        file.write(text)\n",
        "    # print(\"would save a file called \", new_file_name)\n",
        "    return file\n",
        "\n",
        "\n",
        "def generate_files(mean_average, input_path):\n",
        "    \"\"\"\n",
        "        produces the folder with all the needed files inside for comparison\n",
        "    \n",
        "    Args:\n",
        "        mean_average: float, mean multiplier\n",
        "    \"\"\"\n",
        "    dir_name = 'clean_files_' + str(int(mean_average * 100)) + '%_mean'\n",
        "    os.mkdir(dir_name)\n",
        "    corpus_path = input_path\n",
        "    for _, _, files in os.walk(corpus_path, topdown=False):\n",
        "        for name in files:\n",
        "            if name.endswith(\".txt\"):\n",
        "                full_name = corpus_path + '/' + name\n",
        "                file = rd.extract_text(full_name, mean_average)\n",
        "                os.rename(file.name, dir_name + '/' + file.name)\n",
        "                \n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sampple file cleaning execution that will give you three adjusted corpuses based on the percentile mean reduction\n",
        "# NOTE: THE INPUT PATH MUST BE UPDATED TO THE RESPECTIVE CORPUS OF SUSPICIOUS PASSAGES\n",
        "\n",
        "# input_path = \"./31.Suspicious_Passages/Corpus\"\n",
        "    \n",
        "# generate_files(.25, input_path)\n",
        "# generate_files(.5, input_path)\n",
        "# generate_files(.75, input_path)"
      ],
      "metadata": {
        "id": "8gMp3ol3TjIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN FILES GENERATOR"
      ],
      "metadata": {
        "id": "YFVJmMVqtfKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN FILE GENERATOR\n",
        "\n",
        "def generate_files(mean_average, path):\n",
        "    \"\"\"\n",
        "        produces the folder with all the needed files inside for comparison\n",
        "    \n",
        "    Args:\n",
        "        mean_average: float, mean multiplier\n",
        "    \"\"\"\n",
        "    dir_name = 'clean_files_' + str(int(mean_average * 100)) + '%_mean'\n",
        "    os.mkdir(dir_name)\n",
        "    corpus_path = path\n",
        "    for _, _, files in os.walk(corpus_path, topdown=False):\n",
        "        for name in files:\n",
        "            if name.endswith(\".txt\"):\n",
        "                full_name = corpus_path + '/' + name\n",
        "                file = rd.extract_text(full_name, mean_average)\n",
        "                os.rename(file.name, dir_name + '/' + file.name)\n",
        "\n",
        "\n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sampple clean file generator execution\n",
        "# NOTE: THE INPUT PATH MUST BE UPDATED TO THE RESPECTIVE CORPUS OF SUSPICIOUS PASSAGES      \n",
        "\n",
        "# path = \"31.Suspicious_Passages/corpus\"\n",
        "\n",
        "# generate_files(.25, path)\n",
        "# generate_files(.5, path)\n",
        "# generate_files(.75, path)\n"
      ],
      "metadata": {
        "id": "ucDgnKOYtRTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIMILARITY COMPUTATION - THRESHOLD"
      ],
      "metadata": {
        "id": "QKNyOtO-t-gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMILIARITY COMPUTATION - BINARY OUTPUT AND FOREST OUTPUT BASED ON THRESHOLD\n",
        "\n",
        "\n",
        "def stemm_vocabulary(rawdata_list):\n",
        "    \"\"\"takes the raw with the following collumns:\n",
        "        [file_name, line_counter, unique_vocab, vocabulary,  unique_char, characters, text])\n",
        "        and extracts the vocabulary for stemming which is then passed onto the shingling.\n",
        "    Args:\n",
        "        dataframe_list (list(pandas.DataFrame)): takes a list of previously computed dataframes and extracts the list of vocbularies\n",
        "    Returns:\n",
        "        list: returns a list where the vocabulary (column index 3) has been stemmed\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_vocab_list = rawdata_list.copy()\n",
        "    stemmed_vocab = [stemmer.stem(vocab) for vocab in stemmed_vocab_list]\n",
        "    return stemmed_vocab\n",
        "\n",
        "\n",
        "def k_shingle(stemmed_voc, k):\n",
        "    \n",
        "    \"\"\"\n",
        "    Creates a shingle set based on the k amount from a list of strings\n",
        "    Args:\n",
        "        stemmed_voc (list of strings): a list of stemmed vocabulary\n",
        "    Returns:\n",
        "         Set: sets of shingles\n",
        "    \"\"\"\n",
        "    # Recreate original text with stemmed vocab\n",
        "    txt = \"\"\n",
        "    for s in stemmed_voc:\n",
        "        txt += \" \" + s\n",
        "\n",
        "    txt.strip()\n",
        "    shingles = ks.shingleset_range(txt, n_min=k, n_max=k)\n",
        "        \n",
        "    return shingles\n",
        "\n",
        "\n",
        "def shingle_set(rootpath, k):\n",
        "    \"\"\"\n",
        "    Obtains a set of files in a path,\n",
        "    produces a list of set of shingles\n",
        "    :param rootpath: (string) the path of a directory\n",
        "    :param k: (integer) the size of every shingle\n",
        "    :return shingle_all: (list of sets of strings) list of set per document\n",
        "    \"\"\"\n",
        "    list_of_paths = get_paths_from_current_dir(rootpath)\n",
        "    list_of_stemmed = []\n",
        "    shingle_all = []\n",
        "\n",
        "    for elem in list_of_paths:\n",
        "        _, _, vocabulary,  _, _, _ = text_processor(open(elem))\n",
        "        stemmed_vocab = stemm_vocabulary(vocabulary)\n",
        "        list_of_stemmed.append(stemmed_vocab)\n",
        "        \n",
        "\n",
        "    for elem in list_of_stemmed:\n",
        "        shingle_all.append(k_shingle(elem, k))\n",
        "\n",
        "        print(\"In shingle set - this will take some time\")\n",
        "    \n",
        "    return shingle_all\n",
        "    \n",
        "\n",
        "def get_min_hashes(shingle_list, nperm=128):\n",
        "    \"\"\"\n",
        "    Creates and returns the minHash based off of the passed shingle_list\n",
        "    :param shingle_list: (list of sets of shingles)\n",
        "    :return min_hashes: (list of MinHash Objects) a list of MinHashes\n",
        "    \"\"\"\n",
        "    min_hashes = []\n",
        "    for _ in shingle_list:\n",
        "        x = MinHash(num_perm=nperm)\n",
        "        min_hashes.append(x)\n",
        "\n",
        "    for i, shingl_list in enumerate(shingle_list):\n",
        "        for shingle in shingl_list:\n",
        "            min_hashes[i].update(shingle.encode('utf8'))\n",
        "        \n",
        "        print(\"In minHashed - this will take some time\")\n",
        "\n",
        "    return min_hashes\n",
        "\n",
        "\n",
        "def get_lsh(shingle_list, hash_list=None, threshold=.8, num_perms=128, list_names = None):\n",
        "    \"\"\"\n",
        "    Produces, initializes and gets the LSH Object according to the arguments\n",
        "    :param shingle_list: (list of sets of shingles)\n",
        "    :param hash_list: (list of MinHashes)\n",
        "    :param threshold: (float) threshold amount for similarity\n",
        "    :param num_perms: (integer) number of permutations\n",
        "    :return lsh: (LSH Object) the LSH Object\n",
        "    :return min_hashes: (list of MinHash Objects) a list of MinHashes\n",
        "    \"\"\"\n",
        "    min_hashes = hash_list\n",
        "\n",
        "    # If not to provided with min hashes\n",
        "    if hash_list is None:\n",
        "        shingle_list = [list(x) for x in shingle_list]\n",
        "        min_hashes = get_min_hashes(shingle_list, num_perms)\n",
        "\n",
        "    lsh = MinHashLSH(threshold, num_perm=num_perms)\n",
        "    name = list_names if list_names != None else [\"m\"] * len(min_hashes)\n",
        "\n",
        "    for i, my_hash in enumerate(min_hashes):\n",
        "        if name[i] == 'm':\n",
        "            name[i] = 'm'+str(i)\n",
        "        lsh.insert(name[i], my_hash)\n",
        "\n",
        "        print(\"In get LSH - this will take some time\")\n",
        "        \n",
        "    return lsh, min_hashes\n",
        "\n",
        "\n",
        "def get_lsh_data_binary(directory, threshold=.5, shingle_size=5, list_names = None, num_perms=128):\n",
        "    \"\"\"\n",
        "    \n",
        "    Getter method to obtain the lsh Objects created based off of the directory and specified threshold\n",
        "    :param directory: (string) the path of the directory ot be explored\n",
        "    :param threshold: (float) threshold amount for similarity\n",
        "    :return my_lsh: (LSH Object) the lsh Object\n",
        "    :return my_lsh_forest: (LSHForest Object) the lshForest Object\n",
        "    :return my_hashes: (list of MinHash Objects) the list of all MinHashes used for later querying\n",
        "    \"\"\"\n",
        "\n",
        "    shingles = shingle_set(directory, shingle_size)                       \n",
        "    my_lsh, my_hashes = get_lsh(shingle_list=shingles, threshold=threshold, num_perms=num_perms, list_names = list_names)  \n",
        "    return my_lsh, my_hashes\n",
        "\n",
        "\n",
        "def compute_plagiarism_binary(directory, thresh = 0.3, shingle_size=7, num_perms=128 ):\n",
        "    \"\"\"Outputs the binary dataframe in binary numbers whether a document has a suspicious passage\n",
        "    compared to the other documents that are within the corpus. \n",
        "\n",
        "    Args:\n",
        "        directory (string): takes a directory to all the documents to be compared\n",
        "        top_k (int, optional): _description_. Defaults to 2.\n",
        "        thresh (float, optional): _description_. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    list_paths = get_paths_from_current_dir(directory)\n",
        "    names = [x.split(\"/\")[-1].split(\".\")[0] for x in list_paths]\n",
        "    my_lsh, my_hashes = get_lsh_data_binary(directory, threshold=thresh, list_names = names, shingle_size=shingle_size, num_perms=num_perms)\n",
        "    \n",
        "    global_lsh = []\n",
        "    \n",
        "    for i, elem in enumerate(my_hashes):\n",
        "        global_lsh.append([names[i], my_lsh.query(elem)])\n",
        "    \n",
        "    # DataFrame for LSH\n",
        "    df_LSH_Binary = pd.DataFrame(global_lsh, columns=[\"DocumentName\", \"Similar Files - Threshold\"])\n",
        "    #df_LSH_Binary.to_csv(directory + 'LSHThreshold03_s5.csv')\n",
        "    df_LSH_Binary.to_csv(directory + 'LSHThreshold' + str(thresh*10) + '_Ksize' + str(shingle_size) + \"_nperm\" + str(num_perms) +'.csv')\n",
        "    \n",
        "    df_LSH_Threshold = df_LSH_Binary.copy()\n",
        "    df1 = df_LSH_Binary[\"Similar Files - Threshold\"].apply(lambda x: 1 if len(x) > 1 else 0)\n",
        "    df_LSH_Binary[\"Similar Files - Threshold\"] = df1\n",
        "    \n",
        "    binary_file_name = directory + 'LSHbinary' + '_T0' + str(thresh*10) + '_Ksize' + str(shingle_size) + \"_nperm\" + str(num_perms) +'.csv'\n",
        "    df_LSH_Binary.to_csv(binary_file_name)\n",
        "    \n",
        "    print(\"DATA FRAME BASED ON THRESHOLD\")\n",
        "    print(df_LSH_Threshold)\n",
        "    print(\"-\"*50)\n",
        "    print(\"DATA FRAME BASED ON THRESHOLD - Binary\")\n",
        "    print(df_LSH_Binary)\n",
        "    print(\"-\"*50)\n",
        "    \n",
        "    return df_LSH_Threshold, df_LSH_Binary, binary_file_name\n",
        "\n",
        " \n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sample compute plagiarism execution that will give you the LSH output in a binary format and in a forest format based on a threshold\n",
        "# NOTE: THE INPUT PATH MUST BE UPDATED TO THE RESPECTIVE CORPUS OF SUSPICIOUS PASSAGES\n",
        "\n",
        "# input_path = \"./31.Suspicious_Passages/Corpus\"\n",
        "    \n",
        "# compute_plagiarism_binary(input_path, thresh = 0.3, shingle_size = 8, num_perms = 128)"
      ],
      "metadata": {
        "id": "m98-civKVskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIMILIARITY COMPUTATION - TOP-K"
      ],
      "metadata": {
        "id": "8QOzgmqKuSO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMILIARITY COMPUTATION - BINARY OUTPUT AND FOREST OUTPUT BASED ON TOP-K AND THRESHOLD\n",
        "\n",
        "def get_lsh_forest(hash_list, num_perms=128, list_names=None):\n",
        "    \"\"\"\n",
        "    Produces, initializes and gets the LSHForest Object according to the arguments\n",
        "    :param hash_list: (list of MinHashes)\n",
        "    :param num_perms: (integer) number of permutations\n",
        "    :return lsh: (LSH Object) the LSHForest Object\n",
        "    \"\"\"\n",
        "    lsh = MinHashLSHForest(num_perm=num_perms)\n",
        "\n",
        "    if list_names == None:\n",
        "        names = [\"m\"] * len(hash_list)\n",
        "        names = [names[i] + str(i) for i in range(len(names))]\n",
        "    else:\n",
        "        names = list_names\n",
        "\n",
        "    for i, my_hash in enumerate(hash_list):\n",
        "        lsh.add(names[i], my_hash)\n",
        "    lsh.index()\n",
        "\n",
        "    return lsh\n",
        "\n",
        "\n",
        "def get_lsh_data(directory=\"test_dir\", threshold=.5, shingle_size=3, n_perm=128):\n",
        "    \"\"\"\n",
        "    Getter method to obtain the lsh Objects created based off of the directory and specified threshold\n",
        "    :param directory: (string) the path of the directory ot be explored\n",
        "    :param threshold: (float) threshold amount for similarity\n",
        "    :return my_lsh: (LSH Object) the lsh Object\n",
        "    :return my_lsh_forest: (LSHForest Object) the lshForest Object\n",
        "    :return my_hashes: (list of MinHash Objects) the list of all MinHashes used for later querying\n",
        "    \"\"\"\n",
        "\n",
        "    names = [x.split(\"/\")[-1].split(\".\")[0] for x in get_paths_from_current_dir(directory)]\n",
        "    print(\"names\", names)\n",
        "    shingles, united_shingles = shingle_set(directory, shingle_size, names)  # Get Shingles\n",
        "    my_lsh, my_hashes = get_lsh(shingle_list=shingles, threshold=threshold, num_perms=n_perm,\n",
        "                                list_names=names)  # Get lsh and common hashes\n",
        "    my_lsh_forest = get_lsh_forest(hash_list=my_hashes, num_perms=n_perm,\n",
        "                                   list_names=names)  # Get lsh forest with hashes\n",
        "\n",
        "    return my_lsh, my_lsh_forest, my_hashes, names\n",
        "\n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sample LSH Forest execution that will give you the LSH outputin a forest format based on a given top k and threshold\n",
        "# NOTE: THE INPUT PATH MUST BE UPDATED TO THE RESPECTIVE CORPUS OF SUSPICIOUS PASSAGES\n",
        "\n",
        "# input_path = \"./31.Suspicious_Passages/corpus\"\n",
        "\n",
        "# my_lsh, my_lsh_forest, my_hashes, names = get_lsh_data(input_path, threshold=.6, shingle_size=4,\n",
        "#                                                 n_perm=128)  # Get lsh, forest and hashes\n",
        "\n",
        "# LSH_Model = my_lsh\n",
        "# LSH_FOREST_Model = my_lsh_forest\n",
        "# print(\"regular LSH:\", my_lsh.query(my_hashes[0]))           # Get documents above threshold\n",
        "# print(\"forest LSH:\", my_lsh_forest.query(my_hashes[0], 2))  # Get k document close to hash[x]\n",
        "\n",
        "# print(\"Similar Elements based on Threshold of .6 and shingle size 4\")\n",
        "# for i in range(len(my_hashes)):\n",
        "#     print(names[i], \"-> Similar Items ->\", my_lsh.query(my_hashes[i]))\n",
        "\n"
      ],
      "metadata": {
        "id": "3EOKa0FUXfOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION"
      ],
      "metadata": {
        "id": "YlLtaYBNuayn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "\n",
        "# IF DIFFERENT CORPUS USED, USE DIFFERENT GROUND TRUTH PATH\n",
        "ground_truth_path = \"./31.Suspicious_Passages/ground_truth.tsv\"\n",
        "\n",
        "def get_ground_truth(gt_path):\n",
        "    \"\"\" Gets the ground truth and transforms it into an array\n",
        "\n",
        "    Returns:\n",
        "        Array with two elements: Returns a list with the file name and its classification of being plagiarised or not.\n",
        "        1 if plagiarised else not. \n",
        "    \"\"\"\n",
        "    path = gt_path\n",
        "\n",
        "    df = pd.read_csv(path, sep='\\t')\n",
        "    return df.to_numpy()\n",
        "\n",
        "\n",
        "def pop_confusion_metrics(tt_path, gt_path = \"./31.Suspicious_Passages/ground_truth.tsv\"):\n",
        "    \"\"\"\n",
        "    Populates the confusion matrix based on true positive, flase postive, false negative and false positive. \n",
        "\n",
        "    Args:\n",
        "        path (String): takes the path to the test truth to then compare to the ground truth\n",
        "\n",
        "    Returns:\n",
        "        int, int, int, int: returns true positve, false positive, false negative, true negative\n",
        "    \"\"\"\n",
        "    gt = get_ground_truth(gt_path)\n",
        "    \n",
        "    # Transform both sets to dictionaries for O(n) comparison\n",
        "    gt_dict = {}\n",
        "    \n",
        "    for elem in gt:\n",
        "        gt_dict[elem[0]] = elem[1]\n",
        "    \n",
        "    test_truth = binarycsv_transform(tt_path)\n",
        "    \n",
        "    test_dict = {}\n",
        "    for elem in test_truth:\n",
        "        test_dict[elem[0].split(\"-\")[-1]] = elem[1]\n",
        "    \n",
        "    # Comparison to compute false positive, false negative, true positive and true negative\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for elem in gt_dict:\n",
        "        actual = int(gt_dict[elem])\n",
        "        predicted = int(test_dict[elem])\n",
        "        if actual == 1:\n",
        "            if predicted == 0:\n",
        "                FN += 1\n",
        "            elif predicted == 1:\n",
        "                TP += 1\n",
        "        elif actual == 0:\n",
        "            if predicted == 0:\n",
        "                TN += 1\n",
        "            elif predicted == 1:\n",
        "                FP += 1\n",
        "                \n",
        "    print(\"TP is\", TP)\n",
        "    print(\"FP is\", FP)\n",
        "    print(\"FN is\", FN)\n",
        "    print(\"TN is\", TN)\n",
        "    \n",
        "    return TP, FP, FN, TN\n",
        "\n",
        "\n",
        "def binarycsv_transform(path):\n",
        "    \"\"\"\n",
        "    Takes the test truth and transforms it into an array\n",
        "\n",
        "    Args:\n",
        "        path (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    csv_array = []\n",
        "    with open(path, 'r') as file:\n",
        "        csvreader = csv.reader(file)\n",
        "        for row in csvreader:\n",
        "            csv_array.append([row[1], row[2]])\n",
        "    \n",
        "    return csv_array[1:]\n",
        "\n",
        "def prec_acc_comp(tt_path, gt_path):\n",
        "    \"\"\"\n",
        "    Calculates precision, recall, accuracy\n",
        "\n",
        "    Args:\n",
        "        path (String): takes the path to the test truth to then compare to the ground truth\n",
        "\n",
        "    Returns:\n",
        "        int, int, int: precision, recall, accuracy\n",
        "    \"\"\"\n",
        "    TP, FP, FN, TN = pop_confusion_metrics(tt_path, gt_path)\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
        "    F1 = (2*precision*recall)/(recall + precision)\n",
        "    return str(round(precision*100, 2))+'%', \\\n",
        "        str(round(recall*100, 2))+'%', \\\n",
        "        str(round(acc*100, 2))+'%' , \\\n",
        "        str(round(F1*100, 2))+'%'\n",
        "        \n",
        "    \n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sample execution to evaluate the precision, recall and accuracy of our model\n",
        "# NOTE: THE INPUT PATH MUST BE UPDATED TO THE RESPECTIVE SAVED BINARY OUTPUT FILE\n",
        "\n",
        "# path = ./Suspicious_Passages/corpusLSHbinary_T03.0_Ksize6_nperm128.csv\n",
        "\n",
        "    \n",
        "# print(\"03 THRESHOLD, ksize 8, nperm 128\")\n",
        "# prec, rec, acc = prec_acc_comp3(path)\n",
        "# print(\"|PREC\", prec, \"|REC:\", rec, \"|ACC:\", acc)\n",
        "# print(100 * \"-\")"
      ],
      "metadata": {
        "id": "BHKUt5HebOgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN GROUND TRUTH"
      ],
      "metadata": {
        "id": "-jAmnE1bueoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RECOMPUTATION OF GROUND TRUTH\n",
        "\n",
        "# This section aims to create a better ground truth with respect to files that actually appear in the dataset\n",
        "\n",
        "corpus_path = \"./31.Suspicious_Passages/corpus\"\n",
        "filepath = \"./31.Suspicious_Passages/corpus/suspicious-document00014.xml\"\n",
        "\n",
        "# List of all source-document names\n",
        "dataset_file_names = [doc.split('/')[-1] for doc in get_paths_from_current_dir(corpus_path)]\n",
        "# List of all xml file names\n",
        "xml_path_names = [xml_doc for xml_doc in get_paths_from_current_dir(corpus_path, 'xml')]\n",
        "\n",
        "#References for quick access\n",
        "ground_truth = []\n",
        "plagiarism_table = []\n",
        "speculative_list = []\n",
        "\n",
        "def get_plagiarized_sources(text_name):\n",
        "    \"\"\"\n",
        "    Obtains the list of artificial insertions in the xml file of a document in the corpus\n",
        "    :param text_name: string corresponding to name of the source text path\n",
        "    :return: (list of string) a list of file names linked by plagiarizm to the text path document\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    text = open(text_name)\n",
        "\n",
        "    for line in text:\n",
        "        line = str(line)\n",
        "        if \"artificial-plagiarism\" in str(line):\n",
        "            doc = line.split('source_reference=')[-1].split(' ')[0]\n",
        "            documents.append(doc.strip('\"'))\n",
        "\n",
        "    return list(set(documents))\n",
        "\n",
        "\n",
        "def validate_sources(list_of_sources):\n",
        "    \"\"\"\n",
        "    Checks the real list of sources present in our dataset\n",
        "    :param list_of_sources: the sources that the ground truth indicates as plagiarism for a file\n",
        "    :return: (list of string) a list of file names that are actually in our dataset\n",
        "    \"\"\"\n",
        "    existing_files = dataset_file_names\n",
        "    intersection = set(list_of_sources).intersection(existing_files)\n",
        "    #if len(intersection) > 0:\n",
        "    #    print('plagiarized 1')\n",
        "\n",
        "    return intersection\n",
        "\n",
        "def get_real_ground_truth():\n",
        "    \"\"\"\n",
        "    creates 2 lists representing the new ground truth in the same format as the dataset one,\n",
        "    and additionally a new list which precisely indicates which files are the source of the plagiarism\n",
        "    :return: new_ground_truth (list of string) list of tuples composed of file name and plagiarised index\n",
        "             actual_plagiarism_table (list of string) list of tuples composed of file name and plagiarised sources\n",
        "    \"\"\"\n",
        "    xml_path_names.sort()\n",
        "    document_file_names = [x.split('-')[-1].split('.')[0] for x in dataset_file_names]\n",
        "    document_file_names.sort()\n",
        "\n",
        "    new_ground_truth = []\n",
        "    actual_plagiarism_table = []\n",
        "    for i, xml_path in enumerate(xml_path_names):\n",
        "        plagiarized_list = get_plagiarized_sources(xml_path)\n",
        "        validated_list = validate_sources(plagiarized_list)\n",
        "        actual_plagiarism_table.append([document_file_names[i],\n",
        "                                        [x.split('/')[-1].split('-')[-1].split('.')[0] for x in validated_list]])\n",
        "        new_ground_truth.append([document_file_names[i], 1 if len(validated_list) > 0 else 0])\n",
        "\n",
        "    return new_ground_truth, actual_plagiarism_table\n",
        "\n",
        "def get_speculative_ground(plagiarism_table):\n",
        "    \"\"\"\n",
        "    Creates new list representing a ground truth that flags plagiarism on both elements source and document\n",
        "    to better represent our current reasoning for dealing with candidate pairs\n",
        "    :return: dict_files (dictionary<string, integer>) a dictionary where every element is the file name\n",
        "        and the corresponding plagiarism index\n",
        "    \"\"\"\n",
        "    document_file_names = [x.split('-')[-1].split('.')[0] for x in dataset_file_names]\n",
        "    document_file_names.sort()\n",
        "    dict_files = {}\n",
        "\n",
        "    # First round of setting 1\n",
        "    for elem in plagiarism_table:\n",
        "        dict_files.update({elem[0]: 1 if len(elem[1]) > 0 else 0})\n",
        "\n",
        "    # Second round for setting 1 even to the sources\n",
        "    for elem in plagiarism_table:\n",
        "        for source in elem[1]:\n",
        "            dict_files[source] = 1\n",
        "\n",
        "    return dict_files\n",
        "\n",
        "\n",
        "# EXAMPLE EXECUTION:\n",
        "# No sample execution as the computed ground truths are in the submission folder. \n",
        "\n",
        "# The function was called through the following method and the three tables were computed / created:\n",
        "\n",
        "def execute():\n",
        "    actual_ground_truth, plagiarism_table = get_real_ground_truth()\n",
        "\n",
        "    gt_df = pd.DataFrame(actual_ground_truth, columns=[\"doc_name\", \"plagiarism\"])\n",
        "    gt_df.to_csv(\"actual_ground_truth.tsv\", index=False, sep='\\t')\n",
        "\n",
        "    pl_df = pd.DataFrame(plagiarism_table, columns=[\"doc_name\", \"plagiarism links\"])\n",
        "    pl_df.to_csv(\"plagiarism_links.tsv\", index=False, sep='\\t')\n",
        "\n",
        "    dict_gt = get_speculative_ground(plagiarism_table)\n",
        "    speculative_list = []\n",
        "    [speculative_list.append([elem, dict_gt[elem]]) for elem in dict_gt]\n",
        "\n",
        "    speculative_gt = pd.DataFrame(speculative_list, columns=[\"doc_name\", \"plagiarism\"])\n",
        "    speculative_gt.to_csv(\"speculative_ground_truth.tsv\", index=False, sep='\\t')\n",
        "\n",
        "    return plagiarism_table\n"
      ],
      "metadata": {
        "id": "R_nH6vmSrpdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GROUND TRUTH EVALUATION"
      ],
      "metadata": {
        "id": "jaI5WeZsuySy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth evaluation\n",
        "\n",
        "def actual_gt_eval(gt_path):\n",
        "    \"\"\"\n",
        "    Populates the confusion matrix based on true positive, flase postive, false negative and false positive. \n",
        "\n",
        "    Args:\n",
        "        path (String): takes the path to the test truth to then compare to the ground truth\n",
        "\n",
        "    Returns:\n",
        "        int, int, int, int: returns true positve, false positive, false negative, true negative\n",
        "    \"\"\"\n",
        "    path = gt_path\n",
        "\n",
        "    df = pd.read_csv(path, sep='\\t')\n",
        "    \n",
        "    gt = df.to_numpy()\n",
        "    \n",
        "    gt_dict = {}\n",
        "    \n",
        "    counter_class1 = 0\n",
        "    \n",
        "    for elem in gt:\n",
        "        gt_dict[elem[0]] = elem[1]\n",
        "        if int(elem[1]) == 1:\n",
        "            counter_class1 += 1\n",
        "    print(\"Class 1\", counter_class1)\n",
        "    print(\"'%' class 1\", counter_class1 / len(gt_dict))\n",
        "    \n",
        "# EXAMPLE EXECUTION:\n",
        "# Below is a sample execution to evaluate the actual ground truth\n",
        "# NOTE: the path to the g# EVALUATIONround truth must be adapted to the local directory\n",
        "\n",
        "# gt_path = \"./actual_ground_truth.tsv\"\n",
        "# actual_gt_eval(gt_path)\n"
      ],
      "metadata": {
        "id": "pe03qkGUqr_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXECUTABLE"
      ],
      "metadata": {
        "id": "7KEfdESzvOtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def executable(threshold, k_shingle, num_permutations):\n",
        "  df_LSH_Threshold, df_LSH_Binary, binary_file_name = compute_plagiarism_binary(\"./31.Suspicious_Passages/corpus\", thresh = threshold, shingle_size = k_shingle, num_perms = num_permutations)\n",
        "\n",
        "  path_given_gt = \"./Ground_Truths/ground_truth.tsv\"\n",
        "  path_actual_gt = \"./Ground_Truths/actual_ground_truth.tsv\"\n",
        "  path_speculative_gt = \"./Ground_Truths/speculative_ground_truth.tsv\"\n",
        "\n",
        "  print(\"COMPARED TO GIVEN GROUND TRUTH\")\n",
        "  print(threshold, \" THRESHOLD\", \" ksize: \", k_shingle, \" nperm:\", num_permutations)\n",
        "  prec, rec, acc, f1 = prec_acc_comp(binary_file_name, path_given_gt)\n",
        "  print(\"|PREC\", prec, \"|REC:\", rec, \"|ACC:\", acc, \"|F1:\", f1)\n",
        "  print(100 * \"-\")\n",
        "\n",
        "  print(\"COMPARED TO ACTUAL GROUND TRUTH\")\n",
        "  print(threshold, \" THRESHOLD\", \" ksize: \", k_shingle, \" nperm:\", num_permutations)\n",
        "  prec, rec, acc, f1 = prec_acc_comp(binary_file_name, path_actual_gt)\n",
        "  print(\"|PREC\", prec, \"|REC:\", rec, \"|ACC:\", acc, \"|F1:\", f1)\n",
        "  print(100 * \"-\")\n",
        "\n",
        "  print(\"COMPARED TO SPECULATIVE GROUND TRUTH\")\n",
        "  print(threshold, \" THRESHOLD\", \" ksize: \", k_shingle, \" nperm:\", num_permutations)\n",
        "  prec, rec, acc, f1 = prec_acc_comp(binary_file_name, path_speculative_gt)\n",
        "  print(\"|PREC\", prec, \"|REC:\", rec, \"|ACC:\", acc, \"|F1:\", f1)\n",
        "  print(100 * \"-\")\n",
        "\n",
        "executable(0.3, 8, 128)"
      ],
      "metadata": {
        "id": "BcGQElK8vR1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}